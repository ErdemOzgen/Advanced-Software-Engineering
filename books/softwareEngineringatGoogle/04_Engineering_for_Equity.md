### 4. Engineering for Equity

* Many facial recognition systems are built on face databases from law enforcement. If a particular group is more likely to be arrested, they could appear much more frequently in these datasets, which could lead to biased models
* We need to be aware that there is a problem before we can fix it
* Your system is biased if any inputs to it are biased (e.g. performance ratings when switching teams)
* Measure equity
